{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c8eb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.dataframe td { white-space: nowrap; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "sns.set_theme()\n",
    "\n",
    "# jupyter notebook full-width display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# no text wrapping\n",
    "display(HTML(\"<style>.dataframe td { white-space: nowrap; }</style>\"))\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca3a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base dataframes\n",
    "df_hist = pd.read_pickle('calculations\\df_hist.pickle')\n",
    "df_spec = pd.read_pickle('calculations\\df_spec.pickle')\n",
    "df_arch = pd.read_pickle('calculations\\df_arch.pickle')\n",
    "df_prob = pd.read_pickle('calculations\\df_prob.pickle')\n",
    "df_matching_stats = pd.read_pickle('calculations\\df_matching_stats.pickle')\n",
    "\n",
    "# need to recreate summary with updated calculations\n",
    "# define df_summary\n",
    "df_summary = pd.merge(\n",
    "    df_hist[['sample_id', 'id']].groupby('sample_id').count().rename({'id':'n_hist'}, axis=1),\n",
    "    df_spec[['sample_id', 'id']].groupby('sample_id').count().rename({'id':'n_spec'}, axis=1),\n",
    "    on='sample_id',\n",
    "    how='left'\n",
    ").fillna(0)\n",
    "df_summary.n_spec = df_summary.n_spec.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66925b",
   "metadata": {},
   "source": [
    "# improved bins, plotting and error checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33bef660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    38505\n",
       "8    34192\n",
       "0      242\n",
       "9      231\n",
       "4      228\n",
       "2      225\n",
       "5      224\n",
       "1      207\n",
       "7      206\n",
       "6      187\n",
       "Name: fork_length, dtype: Int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bins used by specimen table\n",
    "# looks like bio data doesn't need to end with a 3 or 8\n",
    "(df_spec.fork_length % 10).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e0caf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      74666\n",
       "fork_length             74447\n",
       "weight                    429\n",
       "river_age               74664\n",
       "notes                   74666\n",
       "sample_id               74666\n",
       "sex_id                      6\n",
       "status_id               74666\n",
       "age_type                74664\n",
       "sweep_id                74666\n",
       "life_stage_id           74666\n",
       "old_id                  74666\n",
       "smart_river_age         74664\n",
       "smart_river_age_type    74664\n",
       "matching_id             74666\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spec.notna().sum()\n",
    "# note: there are only 6 sex data and they are the import error (should be bio) from the other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d4829a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight   0.841\n",
       "sex_id   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check: confirm that values ending in not 3/8 have more weight/sex\n",
    "\n",
    "# still no sex data, but most of the weight data is here \n",
    "# (you would expect exactly 80% if it was 100% because more detailed measurements could include n%5==3)\n",
    "\n",
    "df_spec[df_spec.fork_length % 5 != 3][['weight', 'sex_id']].notna().sum() / df_spec[['weight', 'sex_id']].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29c9108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   74447.000\n",
       "mean       57.040\n",
       "std        23.221\n",
       "min        23.000\n",
       "25%        43.000\n",
       "50%        48.000\n",
       "75%        68.000\n",
       "max       163.000\n",
       "Name: fork_length, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bins go from 23 to 163\n",
    "df_spec.fork_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8379c6",
   "metadata": {},
   "source": [
    "# improved binning and error calculating\n",
    "using bins as utilised by the specimen table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc517fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loop through ascending sort first - pick best match out of either asc or desc sort loop method\n",
    "\n",
    "weight_tolerance = 1\n",
    "potential_fish_matches = []  # list(sample, spec, hist, hist_total) - these should only trigger if an exact match on sex/len/wt within tolerance, if exists\n",
    "strong_sample_matches = list()  # a match is found for every fish in df_hist - sample likely contains duplicated spec/bio\n",
    "bad_sample_matches = set()  # df_hist contains unmatchable fish - some fish are definitely not duplicated spec/bio\n",
    "last_sample = 0\n",
    "df = pd.DataFrame()\n",
    "hist_total, hist_matches = 999, 0 \n",
    "\n",
    "for i, row in df_hist.sort_values(['sample_id', 'id']).iterrows():\n",
    "\n",
    "    fish_id, sample_id, fork_length, weight, sex_id = row[['id', 'sample_id', 'fork_length', 'weight', 'sex_id']]\n",
    "    current_bin = fork_length - fork_length%5, fork_length - fork_length%5 + 5  # these are int bins n%5, could add 0.5 per above note\n",
    "    \n",
    "    if last_sample != sample_id:\n",
    "        df = df_spec[df_spec.sample_id==sample_id]\n",
    "        # strong matches\n",
    "        if hist_matches == hist_total:\n",
    "            strong_sample_matches += [last_sample]\n",
    "        hist_matches = 0\n",
    "        hist_total = df_hist[df_hist.sample_id==sample_id].shape[0]\n",
    "        \n",
    "    if not df.empty:\n",
    "        \n",
    "        results = df[\n",
    "            ((df.fork_length>=current_bin[0]) & (df.fork_length<current_bin[1])) # check if fork_length is in the same bin\n",
    "            & (\n",
    "                ((df.weight>=weight-weight_tolerance) & (df.weight>=weight-weight_tolerance))\n",
    "                | df.weight.isnull()\n",
    "            )\n",
    "            & ((df.sex_id==sex_id) | df.sex_id.isnull())\n",
    "        ]\n",
    "        if not results.empty:\n",
    "            hist_matches += 1\n",
    "            potential_fish_matches += [[sample_id, fish_id, results.iloc[0].id, hist_total, hist_matches, fork_length, results.iloc[[0]].fork_length.values[0]]]\n",
    "            df = df.drop(results.iloc[[0]].index[0]) # drop this row so it doesn't get matched again\n",
    "        else:\n",
    "            bad_sample_matches.add(sample_id)  # triggers if results is empty (there are no matches)\n",
    "\n",
    "    else:\n",
    "        bad_sample_matches.add(sample_id)  # triggers if df is empty\n",
    "\n",
    "    last_sample = sample_id\n",
    "    \n",
    "    \n",
    "# use potential fish matches to calculate error\n",
    "error_penalty_per_unmatched_fish = 100  # arbitrary\n",
    "\n",
    "df_matches_asc = pd.DataFrame(potential_fish_matches, columns=['sample_id', 'hist_id', 'spec_id', 'total_hist', 'cumulative_matches', 'hist_fork_length', 'spec_fork_length'])\n",
    "df_match_counts = df_matches_asc.groupby('sample_id').max()[['cumulative_matches', 'total_hist']].rename({'total_hist':'total', 'cumulative_matches':'matches'}, axis=1)\n",
    "\n",
    "df_matches_asc = pd.merge(\n",
    "    df_matches_asc,\n",
    "    df_match_counts,\n",
    "    on='sample_id',\n",
    "    how='left'\n",
    ").drop(['cumulative_matches', 'total_hist'], axis=1)\n",
    "\n",
    "df_matches_asc['fish_sq_error'] = (df_matches_asc['hist_fork_length'] - df_matches_asc['spec_fork_length']) ** 2\n",
    "df_matches_asc['unmatched_penalty'] = (df_matches_asc['total'] - df_matches_asc['matches']) * error_penalty_per_unmatched_fish\n",
    "df_matches_asc = df_matches_asc.merge(\n",
    "    pd.DataFrame(df_matches_asc[['sample_id', 'fish_sq_error', 'unmatched_penalty']].groupby('sample_id').agg({'fish_sq_error':'sum', 'unmatched_penalty':'max'}).sum(axis=1), columns=['sample_SSE']),\n",
    "    on='sample_id',\n",
    "    how='left'\n",
    ").drop(['fish_sq_error', 'unmatched_penalty'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712830eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 55.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# same calc for descending sort - pick best match out of either method\n",
    "\n",
    "weight_tolerance = 1\n",
    "potential_fish_matches = []  # list(sample, spec, hist, hist_total) - these should only trigger if an exact match on sex/len/wt within tolerance, if exists\n",
    "strong_sample_matches = list()  # a match is found for every fish in df_hist - sample likely contains duplicated spec/bio\n",
    "bad_sample_matches = set()  # df_hist contains unmatchable fish - some fish are definitely not duplicated spec/bio\n",
    "last_sample = 0\n",
    "df = pd.DataFrame()\n",
    "hist_total, hist_matches = 999, 0 \n",
    "\n",
    "for i, row in df_hist.sort_values(['sample_id', 'id'], ascending=False).iterrows():\n",
    "\n",
    "    fish_id, sample_id, fork_length, weight, sex_id = row[['id', 'sample_id', 'fork_length', 'weight', 'sex_id']]\n",
    "    current_bin = fork_length - fork_length%5, fork_length - fork_length%5 + 5  # these are int bins n%5, could add 0.5 per above note\n",
    "    \n",
    "    if last_sample != sample_id:\n",
    "        df = df_spec[df_spec.sample_id==sample_id]\n",
    "        # strong matches\n",
    "        if hist_matches == hist_total:\n",
    "            strong_sample_matches += [last_sample]\n",
    "        hist_matches = 0\n",
    "        hist_total = df_hist[df_hist.sample_id==sample_id].shape[0]\n",
    "        \n",
    "    if not df.empty:\n",
    "        \n",
    "        results = df[\n",
    "            ((df.fork_length>=current_bin[0]) & (df.fork_length<current_bin[1])) # check if fork_length is in the same bin\n",
    "            & (\n",
    "                ((df.weight>=weight-weight_tolerance) & (df.weight>=weight-weight_tolerance))\n",
    "                | df.weight.isnull()\n",
    "            )\n",
    "            & ((df.sex_id==sex_id) | df.sex_id.isnull())\n",
    "        ]\n",
    "        if not results.empty:\n",
    "            hist_matches += 1\n",
    "            potential_fish_matches += [[sample_id, fish_id, results.iloc[0].id, hist_total, hist_matches, fork_length, results.iloc[[0]].fork_length.values[0]]]\n",
    "            df = df.drop(results.iloc[[0]].index[0]) # drop this row so it doesn't get matched again\n",
    "        else:\n",
    "            bad_sample_matches.add(sample_id)  # triggers if results is empty (there are no matches)\n",
    "\n",
    "    else:\n",
    "        bad_sample_matches.add(sample_id)  # triggers if df is empty\n",
    "\n",
    "    last_sample = sample_id\n",
    "    \n",
    "    \n",
    "# use potential fish matches to calculate error\n",
    "error_penalty_per_unmatched_fish = 100  # arbitrary\n",
    "\n",
    "df_matches_desc = pd.DataFrame(potential_fish_matches, columns=['sample_id', 'hist_id', 'spec_id', 'total_hist', 'cumulative_matches', 'hist_fork_length', 'spec_fork_length'])\n",
    "df_match_counts = df_matches_desc.groupby('sample_id').max()[['cumulative_matches', 'total_hist']].rename({'total_hist':'total', 'cumulative_matches':'matches'}, axis=1)\n",
    "\n",
    "df_matches_desc = pd.merge(\n",
    "    df_matches_desc,\n",
    "    df_match_counts,\n",
    "    on='sample_id',\n",
    "    how='left'\n",
    ").drop(['cumulative_matches', 'total_hist'], axis=1)\n",
    "\n",
    "df_matches_desc['fish_sq_error'] = (df_matches_desc['hist_fork_length'] - df_matches_desc['spec_fork_length']) ** 2\n",
    "df_matches_desc['unmatched_penalty'] = (df_matches_desc['total'] - df_matches_desc['matches']) * error_penalty_per_unmatched_fish\n",
    "df_matches_desc = df_matches_desc.merge(\n",
    "    pd.DataFrame(df_matches_desc[['sample_id', 'fish_sq_error', 'unmatched_penalty']].groupby('sample_id').agg({'fish_sq_error':'sum', 'unmatched_penalty':'max'}).sum(axis=1), columns=['sample_SSE']),\n",
    "    on='sample_id',\n",
    "    how='left'\n",
    ").drop(['fish_sq_error', 'unmatched_penalty'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b83865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>SSE_asc</th>\n",
       "      <th>SSE_desc</th>\n",
       "      <th>delta</th>\n",
       "      <th>delta_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000</td>\n",
       "      <td>768.000</td>\n",
       "      <td>768.000</td>\n",
       "      <td>768.000</td>\n",
       "      <td>768.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6406.895</td>\n",
       "      <td>535.060</td>\n",
       "      <td>527.673</td>\n",
       "      <td>7.387</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1345.255</td>\n",
       "      <td>773.377</td>\n",
       "      <td>738.046</td>\n",
       "      <td>67.110</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4404.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-614.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7169.500</td>\n",
       "      <td>274.000</td>\n",
       "      <td>302.500</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>7862.650</td>\n",
       "      <td>1879.300</td>\n",
       "      <td>1789.700</td>\n",
       "      <td>81.000</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5%</th>\n",
       "      <td>7928.825</td>\n",
       "      <td>2988.475</td>\n",
       "      <td>2824.775</td>\n",
       "      <td>122.775</td>\n",
       "      <td>0.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.5%</th>\n",
       "      <td>7990.165</td>\n",
       "      <td>4289.610</td>\n",
       "      <td>4099.050</td>\n",
       "      <td>232.310</td>\n",
       "      <td>1.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8001.000</td>\n",
       "      <td>6865.000</td>\n",
       "      <td>6548.000</td>\n",
       "      <td>336.000</td>\n",
       "      <td>1.646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id  SSE_asc  SSE_desc    delta  delta_scaled\n",
       "count    768.000  768.000   768.000  768.000       768.000\n",
       "mean    6406.895  535.060   527.673    7.387         0.061\n",
       "std     1345.255  773.377   738.046   67.110         0.200\n",
       "min     4404.000    0.000     0.000 -614.000         0.000\n",
       "50%     7169.500  274.000   302.500    4.000         0.016\n",
       "95%     7862.650 1879.300  1789.700   81.000         0.190\n",
       "97.5%   7928.825 2988.475  2824.775  122.775         0.769\n",
       "99.5%   7990.165 4289.610  4099.050  232.310         1.429\n",
       "max     8001.000 6865.000  6548.000  336.000         1.646"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are different enough that we should combine the results\n",
    "SSE_comparison = pd.merge(\n",
    "    df_matches_asc.groupby('sample_id').max()['sample_SSE'].reset_index().rename({'sample_SSE':'SSE_asc'}, axis=1),\n",
    "    df_matches_desc.groupby('sample_id').max()['sample_SSE'].reset_index().rename({'sample_SSE':'SSE_desc'}, axis=1),\n",
    "    on='sample_id'\n",
    ")\n",
    "SSE_comparison['delta'] = SSE_comparison['SSE_asc'] - SSE_comparison['SSE_desc']\n",
    "SSE_comparison['delta_scaled'] = (SSE_comparison['delta'] / ((SSE_comparison['SSE_asc'] + SSE_comparison['SSE_desc']) / 2)).abs().fillna(0)\n",
    "SSE_comparison.sort_values('delta_scaled', ascending=False).head(22)\n",
    "SSE_comparison.describe(percentiles=[0.95, 0.975,0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09d7e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine lowest errors by sample number\n",
    "id_asc = SSE_comparison[SSE_comparison.SSE_asc<=SSE_comparison.SSE_desc].sample_id\n",
    "id_desc = SSE_comparison[SSE_comparison.SSE_asc>SSE_comparison.SSE_desc].sample_id\n",
    "\n",
    "df_matches = pd.concat([\n",
    "    df_matches_asc[df_matches_asc.sample_id.isin(id_asc)],\n",
    "    df_matches_desc[df_matches_desc.sample_id.isin(id_desc)]\n",
    "]).sort_values(['sample_id', 'hist_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beebedac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_hist</th>\n",
       "      <th>n_spec</th>\n",
       "      <th>matched_proportion</th>\n",
       "      <th>sample_SSE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.709</td>\n",
       "      <td>94.156</td>\n",
       "      <td>0.841</td>\n",
       "      <td>816.430</td>\n",
       "      <td>330.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39.659</td>\n",
       "      <td>109.540</td>\n",
       "      <td>0.217</td>\n",
       "      <td>1810.935</td>\n",
       "      <td>1745.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>5.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.611</td>\n",
       "      <td>19.000</td>\n",
       "      <td>2.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>0.905</td>\n",
       "      <td>296.000</td>\n",
       "      <td>12.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>78.000</td>\n",
       "      <td>209.800</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1508.400</td>\n",
       "      <td>41.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>184.400</td>\n",
       "      <td>491.720</td>\n",
       "      <td>1.000</td>\n",
       "      <td>9999.000</td>\n",
       "      <td>9999.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>346.000</td>\n",
       "      <td>1016.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>9999.000</td>\n",
       "      <td>9999.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      n_hist   n_spec  matched_proportion  sample_SSE      MSE\n",
       "mean  34.709   94.156               0.841     816.430  330.385\n",
       "std   39.659  109.540               0.217    1810.935 1745.597\n",
       "min    1.000    0.000               0.000       0.000    0.000\n",
       "1%     1.000    0.000               0.000       1.000    0.500\n",
       "10%    5.000    8.000               0.611      19.000    2.388\n",
       "50%   25.000   61.000               0.905     296.000   12.014\n",
       "90%   78.000  209.800               1.000    1508.400   41.390\n",
       "99%  184.400  491.720               1.000    9999.000 9999.000\n",
       "max  346.000 1016.000               1.000    9999.000 9999.000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of findings\n",
    "df_matches['MSE'] = df_matches['sample_SSE'] / df_matches['total']\n",
    "df_matches['matched_proportion'] = df_matches['matches'] / df_matches['total']\n",
    "df_match_error_summary = df_matches.groupby('sample_id').max().sort_values('sample_SSE', ascending=False)[['matched_proportion', 'sample_SSE', 'MSE']]\n",
    "\n",
    "# merge into summary\n",
    "df_summary = df_summary.merge(df_match_error_summary, on='sample_id', how='left')\n",
    "df_summary.loc[df_summary.matched_proportion.isnull() | df_summary.sample_SSE.isnull() | df_summary.MSE.isnull(), ['matched_proportion', 'sample_SSE', 'MSE']] = 0, 9999, 9999\n",
    "\n",
    "df_summary.describe(percentiles=[0.01, 0.1, 0.9, 0.99]).drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96bf34",
   "metadata": {},
   "source": [
    "# Save Summary File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0806fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_pickle('calculations\\df_summary.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
